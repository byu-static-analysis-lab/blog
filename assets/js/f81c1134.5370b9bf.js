"use strict";(self.webpackChunkdev_blog=self.webpackChunkdev_blog||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"db-aware-program-optimization","metadata":{"permalink":"/blog/db-aware-program-optimization","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-db-aware-program-optimization.md","source":"@site/blog/2024-11-db-aware-program-optimization.md","title":"Paper - Database-Aware Program Optimization via Static Analysis","description":"A lot of developer work on database applications goes into making sure the interaction between the application layer and the database layer is efficient. In my experience in industry, this work is mostly done manually, and in an ad-hoc fashion. [@ramachandra2014database] give an account about some ways this can be done manually.","date":"2024-11-18T12:00:00.000Z","tags":[{"inline":true,"label":"paper","permalink":"/blog/tags/paper"},{"inline":true,"label":"optimization","permalink":"/blog/tags/optimization"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"api coalescing","permalink":"/blog/tags/api-coalescing"}],"readingTime":3.36,"hasTruncateMarker":true,"authors":[{"name":"Koby Lewis","title":"Author","url":"https://github.com/plyb","imageURL":"https://github.com/plyb.png","key":"koby","page":null}],"frontMatter":{"slug":"db-aware-program-optimization","title":"Paper - Database-Aware Program Optimization via Static Analysis","date":"2024-11-18T12:00","authors":["koby"],"tags":["paper","optimization","database","api coalescing"]},"unlisted":false,"nextItem":{"title":"Paper - A General Fine-Grained Reduction Theory for Effect Handlers","permalink":"/blog/fine-grained-reduction-theory-for-effect-handlers"}},"content":"A lot of developer work on database applications goes into making sure the interaction between the application layer and the database layer is efficient. In my experience in industry, this work is mostly done manually, and in an ad-hoc fashion. [@ramachandra2014database] give an account about some ways this can be done manually.\\n\\n\x3c!--truncate--\x3e\\n\\nThere are # major methods they talk about in the paper. They are as follows:\\n1. Set-oriented query execution: this technique gathers multiple queries together (they specifically reference queries made in a loop) and combines them into a single query whose results can be broken up and used as if they were single queries. Their technique relies on a program transformation which breaks the loop into two loops, one of which adds the individual queries to the batch, and one which consumes the results of that batch.\\n2. Asynchronous Prefetching: this technique tries to move queries as early as possible in the program, so that you can overlap the network latency and computation at the database layer with the computation on the application layer. Optionally, you can batch these queries using technique 1. They also discuss ways to do prefetching in more complicated scenarios, such as when the query is made inside nested procedures.\\n\\n## Koby\'s thoughts\\n\\nThis paper identifies precisely the problem that I\'ve been thinking about, which is how to optimize programs when you know things both about the program and the database schema. More generally, if you have some way to combine api calls that can reduce overhead of some kind, can we find an automated way to do so? However, the paper relies on two broad techniques to solve this: program transformations and (database-aware) compiler optimizations, both of which I see problems with.\\n\\nWhile, generally speaking, program transformations may be alright, I feel that those presented in the paper, such as the loop fission technique that is at the core of almost everything they discuss, are very invasive. They would end up producing code that is difficult to read and maintain (especially if multiple devs implement separate loop fissions, leading to nested loops). The authors themselves point out that this is especially a problem when dealing with queries made in nested procedures. (On the topic of nested procedures, they don\'t describe the full technique, but instead refer to an earlier paper, which I am curious to read).\\n\\nCompiler optimizations are also risky because they make implicit a very important aspect of the program, the algorithms that determine its speed. If these techniques are not universally (or nearly universally) applicable, it would be easy for an inexperienced developer to make a change that causes a serious performance regression without ever realizing. Why am I worried about this and not other compiler optimizations? My ideas are not fully formed on this yet, but I think one aspect is just how important things like network latency are to the performance of an application.\\n\\nSo an ideal technique for me would be one that leaves the code mostly intact as the developer wrote it (which, hopefully, would mean it would be more readable and maintainable), but which also has an explicit reference at some point to how the optimization is taking place. It would also be very difficult to accidentally cause a serious performance regression in the technique, or at least there would be a way to warn the developer if they have.\\n\\nA few thoughts about where to go from here:\\n1. CFA seems like a promising technique to help with the nested procedure problem, but I\'d like to read more about their solution first.\\n2. Algebraic effects could provide a really solid way to provide for batching while still leaving a trace in the code that isn\'t too intrusive. However, I can\'t think of how you could do prefetching using algebraic effects.\\n3. The paper mentions that their techniques could potentially be broadened to other domains where significant overhead exists besides databases. It could be interesting to explore a technique that could generalize to all of these."},{"id":"fine-grained-reduction-theory-for-effect-handlers","metadata":{"permalink":"/blog/fine-grained-reduction-theory-for-effect-handlers","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-fine-grained-reduction-theory-for-effect-handlers.md","source":"@site/blog/2024-11-fine-grained-reduction-theory-for-effect-handlers.md","title":"Paper - A General Fine-Grained Reduction Theory for Effect Handlers","description":"A General Fine-Grained Reduction Theory for Effect Handlers [@sieczkowskigeneral2023]","date":"2024-11-08T12:00:00.000Z","tags":[{"inline":true,"label":"paper","permalink":"/blog/tags/paper"},{"inline":true,"label":"abstract interpreters","permalink":"/blog/tags/abstract-interpreters"},{"inline":true,"label":"reduction semantics","permalink":"/blog/tags/reduction-semantics"},{"inline":true,"label":"effect handlers","permalink":"/blog/tags/effect-handlers"},{"inline":true,"label":"abstract machine","permalink":"/blog/tags/abstract-machine"}],"readingTime":3.33,"hasTruncateMarker":true,"authors":[{"name":"Tim Whiting","title":"Author","url":"https://github.com/TimWhiting","imageURL":"https://github.com/TimWhiting.png","key":"tim","page":null}],"frontMatter":{"slug":"fine-grained-reduction-theory-for-effect-handlers","title":"Paper - A General Fine-Grained Reduction Theory for Effect Handlers","date":"2024-11-08T12:00","authors":["tim"],"tags":["paper","abstract interpreters","reduction semantics","effect handlers","abstract machine"]},"unlisted":false,"prevItem":{"title":"Paper - Database-Aware Program Optimization via Static Analysis","permalink":"/blog/db-aware-program-optimization"},"nextItem":{"title":"Paper - Refocusing in Reduction Semantics","permalink":"/blog/refocusing-reduction"}},"content":"A General Fine-Grained Reduction Theory for Effect Handlers [@sieczkowski_general_2023]\\npresents a theory of effect handlers that can be used for term rewriting systems,\\nand provides a few different abstract machines for effect handlers.\\n\\nA significant contribution of this paper I believe is that it provides\\na top down small-step semantics for effect handlers, which is a bit unusual.\\n\\n\x3c!--truncate--\x3e\\n\\nOne disadvantages of most reduction semantics for effect handlers is that\\nthey require being able to capture a reduction context itself as a term in the language.\\nThis is easy to understand in principle, but makes the reduction semantics\\nhard to translate into an abstract machine.\\n\\nThe fine-grained reduction theory presented in this paper makes it easy to apply the\\nnormal refocusing method to come up with an abstract machine for algebraic effects (see [this paper](./2024-11-refocusing-reduction.md)).\\n\\nThe crux of the idea is that continuations can either be built from the top-down or the bottom-up. \\n\\nTypical reduction semantics for algebraic effects capture the reduction context and wrap it in a closure to plug the context. This is effectively a bottom-up approach.\\n\\nFor example in Generalized Evidence Passing for Effect Handlers [@xie_generalized_2021], \\nKoka\'s evaluation rules look as follows:\\n\\n$$\\n\\\\begin{align*}\\n(app)\\\\;\\\\; &(\u03bbx.\\\\; e) v \u27f6 e[x:=v]  \\\\\\\\\\n(handler)\\\\;\\\\; &\\\\texttt{handler}\\\\; h\\\\; f \u27f6 \\\\texttt{handle}\\\\; h\\\\; (f\\\\; ()) \\\\\\\\\\n(return)\\\\;\\\\; &\\\\texttt{handle}\\\\; h\\\\; v \u27f6 v  \\\\\\\\\\n(perform)\\\\;\\\\; &\\\\texttt{handle}\\\\; h\\\\; E[\\\\texttt{perform}\\\\; op\\\\; v] \u27f6 \\n  f\\\\; v\\\\; (\u03bbx.\\\\; \\\\texttt{handle}\\\\; h\\\\; E[x])\\\\; \\\\texttt{iff}\\\\; op \\\\notin bop(E) \u2227 (op \u27fc f)  \\\\in h\\n\\\\end{align*}\\n$$\\n\\nIn this we can see the caputring of $E$ and wrapping up in a lambda to build the continuation.\\nAdditionally we see the nature of `deep` effect handlers where the continuation reinstates\\nthe handler frame on top of the evaluation context.\\n\\nWhile later in the Koka paper they present a monad for building up the continuation piece by piece, this is still a bottom-up treatment and composition of elementary evaluation contexts. \\n\\nInstead the top down treatment presented in this paper, builds the continuation on the way down, and only uses lambda functions.\\n\\n\\nThe abstract machine presented in the paper is pretty straightforward, though as mentioned in the paper it is a `hybrid` machine because the context changes how evaluation proceeds. In particular\\nevaluation changes depending on whether it is in the context of one or more handlers, versus if it is finishing up a computation in the top level continuation.\\n\\nLater they introduce a similarity relation to equate terms in the fine-grained calculus and a normal calculus. This part is pretty involved since the rewriting of handlers in the fine-grained calculus ends up changing a lot of terms, and the capturing of a resumption in the regular calculus introduces an eta expansion for the application. \\n\\n\\nNext they present three core proofs: Simulation, Confluence, and Standardization.\\n\\nSimulation states that it doesn\'t matter whether you use the fine-grained or non-local semantics you get the same result. i.e. fine-grained simulates non-local and vice versa. One step in one either could correspond to many steps in the other, so you have to consider the transitive closure. Additionally the similarity relation is needed since the terms could look different but be syntactically or otherwise equivalent.\\n\\nConfluence of the relation is the diamond property meaning that if an expression reduces to two different expressions, then there exists an expression that each of those expressions can reduce further to.\\n\\nStandardization states the any reduction sequence that reaches head-normal form can be split into a sequence that reaches a normal form followed by an internal reduction sequence. This is tricky with effect handlers due to the hybrid nature of the semantics in which let and lift behave differently depending if a handler surrounds them.\\n\\n\\nUsing those proofs they show that the reduction theory is a foundation for an equational theory which is sound with respect to the non-local semantics. In other words, rewriting pieces of programs using this theory will result in the same program, even if you switch back to a non-local semantics to execute the rest."},{"id":"refocusing-reduction","metadata":{"permalink":"/blog/refocusing-reduction","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-refocusing-reduction.md","source":"@site/blog/2024-11-refocusing-reduction.md","title":"Paper - Refocusing in Reduction Semantics","description":"Refocusing in Reduction Semantics [@danvyrefocusing2004] presents a way to go from","date":"2024-11-08T12:00:00.000Z","tags":[{"inline":true,"label":"paper","permalink":"/blog/tags/paper"},{"inline":true,"label":"abstract interpreters","permalink":"/blog/tags/abstract-interpreters"},{"inline":true,"label":"reduction semantics","permalink":"/blog/tags/reduction-semantics"}],"readingTime":3.435,"hasTruncateMarker":true,"authors":[{"name":"Tim Whiting","title":"Author","url":"https://github.com/TimWhiting","imageURL":"https://github.com/TimWhiting.png","key":"tim","page":null}],"frontMatter":{"slug":"refocusing-reduction","title":"Paper - Refocusing in Reduction Semantics","date":"2024-11-08T12:00","authors":["tim"],"tags":["paper","abstract interpreters","reduction semantics"]},"unlisted":false,"prevItem":{"title":"Paper - A General Fine-Grained Reduction Theory for Effect Handlers","permalink":"/blog/fine-grained-reduction-theory-for-effect-handlers"},"nextItem":{"title":"Topic - Flow Sensitivity","permalink":"/blog/flow-sensitivity"}},"content":"Refocusing in Reduction Semantics [@danvy_refocusing_2004] presents a way to go from\\na reduction semantics to an refocused pre-abstract machine, and eventually to an abstract machine. The paper is a good read after being introduced to reduction semantics and abstract machines, and has a few simple walkthroughs of how to actually apply the theory.\\n\\n\x3c!--truncate--\x3e\\n\\nThe goal of this paper is to systematize the development of abstract machines based on\\na reduction semantics. \\n\\n### Reduction Semantics\\n\\nReduction semantics involve three distinct steps:\\n- `decompose`: rules for breaking apart expressions into a term that can be reduced and its surrounding evaluation contexts (aka reduction contexts) \\n- `reduction` / `contraction`: rules for how a term should be reduced\\n- `plug`: rules for how to recompose a full program expression given an evaluation context and a term\\n\\nIn reduction semantics some terms are considered fully reduced (values), and others can be decomposed. In general a term can be decomposed in many ways, however \\n\\nOften reduction semantics satisfy the property that there is a unique *complete* decomposition. A decomposition is *complete* iff the term it decomposes into a term that is trivially reducible (not a value, and not itself containing a complete decomposition). For example, the call-by-value simple lambda calculus semantics has the following reduction contexts\\n\\n$C ::= [] \\\\mid app(C,t) \\\\mid app(v,C)$\\n\\nwhere $C$ denotes a reduction context, $t$ a term, and $v$ a value, with $[]$ being a hole.\\n\\nCrucially the restriction on the first term of the $app$ being either an evaluation context, or a value makes this decomposition unique and specifies a left-to-right evaluation order.\\n\\n### Deriving An Abstract Machine\\n\\nGiven this property, the paper shows how to derive an abstract machine from it.\\n\\nIntuitively, instead of finding the decomposition and replugging after every step, we can\\ninstead traverse the reduction contexts of the previous decomposition to find the next\\nreduction step and recompose the evaluation contexts to obtain the new evaluation context.\\nIn this way we can merge all intermediate steps of `plug` followed by `reduction`. The paper\\ncalls this the `refocus` step.\\n\\nAfter merging `plug` and `reduction`, the only thing that we need to get rid of is to inline the `refocus` step itself to get a properly tail recursive abstract machine. \\n\\nOf course the paper goes into more detail, and considers two separate `refocus` functions, one\\nwhich applies when the expression reduces to another potential redux, and another when it reduces to a value. \\nAfter inlining both, the differences don\'t exactly matter, since they are now just part of the machine transitions.\\nHowever, it is useful to understand how the machine is derived, since you don\'t always have an abstract machine, but do know how to select the next statement to reduce.\\n\\n## Conclusion\\n\\nThe conclusion of the paper states:\\n\\n> We have presented a structural result about reduction semantics with context-free grammars of values, reduction contexts, and redexes, and satisfying a unique-decomposition property. These conditions are quite general: they hold for deterministic languages and also for oracle-based non-deterministic languages.\\n\\nI appreciate the examples that they showed to make this claim more concrete - instead of solely in terms of context-free grammars, but it is nice to know that there is very little syntactic restrictions, and that the conditions for applying it are rather general.\\n\\n> The construction of the refocus function suggests a convenient definition of the decompose function that directly connects one-step reduction and evaluation in the form of an abstract machine. It also suggests a practical method to obtain a reduction semantics out of an abstract machine.\\n\\nI wish they spent more time connecting this - how to go in the reverse direction. It is also interesting that they don\'t talk about the restriction to `one-step` reductions. Is that a restriction only going in the reverse, or is it a restriction in the forward direction as well.\\n\\n:::note\\nA many step strategy is apparently one in which a whole set of redexes is contracted simultaneously\\n\\nAs such, the restriction makes sense and I believe applies in both directions. What would an abstract machine that performs multiple steps simultaneously even look like? \\n:::"},{"id":"flow-sensitivity","metadata":{"permalink":"/blog/flow-sensitivity","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-flow-sensitivity.md","source":"@site/blog/2024-11-flow-sensitivity.md","title":"Topic - Flow Sensitivity","description":"There are (at least) three different \\"sensitivities\\" an abstract analysis can have:","date":"2024-11-07T19:17:45.000Z","tags":[{"inline":true,"label":"static analysis","permalink":"/blog/tags/static-analysis"},{"inline":true,"label":"flow analysis","permalink":"/blog/tags/flow-analysis"},{"inline":true,"label":"abstract interpretation","permalink":"/blog/tags/abstract-interpretation"}],"readingTime":1.985,"hasTruncateMarker":true,"authors":[{"name":"Koby Lewis","title":"Author","url":"https://github.com/plyb","imageURL":"https://github.com/plyb.png","key":"koby","page":null}],"frontMatter":{"slug":"flow-sensitivity","title":"Topic - Flow Sensitivity","data":"2024-11-07T12:00","authors":["koby"],"tags":["static analysis","flow analysis","abstract interpretation"]},"unlisted":false,"prevItem":{"title":"Paper - Refocusing in Reduction Semantics","permalink":"/blog/refocusing-reduction"},"nextItem":{"title":"Topic - Abstract Interpretation","permalink":"/blog/abstract-interpretation"}},"content":"There are (at least) three different \\"sensitivities\\" an [abstract analysis](./2024-11-abstract-interpretation.md) can have:\\n- flow insensitive\\n- flow sensitive\\n- path sensitive\\n\\nThis article briefly describes the difference between them.\\n\\n\x3c!--truncate--\x3e\\n\\nOne form of abstract analysis is a type check.\\n\\nConsider this typescript program:\\n```ts\\nfunction myFunc(x : string | number) {\\n  let y;\\n  if (x === 0) {\\n    y = 0; // a\\n  } else {\\n    y = \\"some string\\"; // b\\n  }\\n\\n  if (x === 0) {\\n    console.log(y); // c\\n  } else {\\n    console.log(y + y); // d\\n  }\\n}\\n```\\n\\nWhat is the type of `x` at point `a` in the program? Many type checkers would simply report `string | number`, but TypeScript\'s type checker is smart enough to know that it is `number` here, based on the check `x === 0`, which can only be true if `x` is a number. This is because TypeScript\'s type checker is *flow sensitive*, while many other type checkers are *flow insensitive*.\\n\\nNow, what is the type of `y` at point `c` in the program? We can inspect it and know that if we got to point `c`, then `x === 0` is true, and earlier we must have gone through point `a`. This means that `y` must be `number`, since it was assigned `0`. However, TypeScript still just reports `number | string`. This is because, TypeScript\'s type checker is *not* path-sensitive. Our analysis that we did manually *was* path-sensitive.\\n\\nSo, in order of specificity, we have flow-insensitive < flow-sensitive < path-sensitive. Flow-insensitive analyses collect facts that are true at any point in the program. It is always true to say that `x : string | number` and `y : string | number`. Flow-sensitive analyses associate facts with particular points in the program, but do not distinguish how you got there. At point `c` in the program, it is always true to say that `x : number` and `y : string | number` (we can\'t say anything more specific about `y` because we don\'t know if we went through point `a` or `b`). Path-sensitivity associates facts with points in the program *and how you got there*, or \\"path\\". It is always true to say that at point `c`, given you went through point `a`, that `x : number` and `y : number`, and there are no flows that reach point `c` except those that also previously reach `a`."},{"id":"abstract-interpretation","metadata":{"permalink":"/blog/abstract-interpretation","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-abstract-interpretation.md","source":"@site/blog/2024-11-abstract-interpretation.md","title":"Topic - Abstract Interpretation","description":"Abstract interpretation is a very common technique for analyzing programs. This is a quick primer on the topic.","date":"2024-11-07T12:00:00.000Z","tags":[{"inline":true,"label":"static analysis","permalink":"/blog/tags/static-analysis"},{"inline":true,"label":"abstract interpretation","permalink":"/blog/tags/abstract-interpretation"}],"readingTime":3.17,"hasTruncateMarker":true,"authors":[{"name":"Koby Lewis","title":"Author","url":"https://github.com/plyb","imageURL":"https://github.com/plyb.png","key":"koby","page":null}],"frontMatter":{"slug":"abstract-interpretation","title":"Topic - Abstract Interpretation","date":"2024-11-07T12:00","authors":["koby"],"tags":["static analysis","abstract interpretation"]},"unlisted":false,"prevItem":{"title":"Topic - Flow Sensitivity","permalink":"/blog/flow-sensitivity"},"nextItem":{"title":"Paper - Galois Transformers and Modular Abstract Interpreters","permalink":"/blog/galois-transformers"}},"content":"Abstract interpretation is a very common technique for analyzing programs. This is a quick primer on the topic.\\n\\nThe core idea of an abstract interpretation is to \\"run the program\\", but instead of typical program states/values/etc (sometimes referred to collectively as configurations), we use some replacement that means the execution is decidable (the program always terminates).\\n\\n\x3c!--truncate--\x3e\\n\\nProbably the single most common automated analysis is a type check, which is itself a form of abstract analysis. Throughout this primer, we\'ll use a simply typed lambda calculus, extended with `nat`, `if0`, and `letrec`. Execution will be done via the standard syntax-based transition system with replacement for function application (no environment or store).\\n\\nIn abstract interpretion, the convention is to write the concrete types with their normal notation, and the abstract versions with the normal notation with a hat. In our example, we have the type $Value$, which is the type of the result of running the program, and can either be a value in `nat`, or a closure. We also have type $Type$, which is the result of running the type check. But in the typical abstract interpretation, we would write $\\\\widehat{Value}$. To familiarize us with this notation, I will use the $\\\\widehat{Value}$ notation, but know that it is the same as $Type$.\\n\\nNow, we have two domains, $Value$ and $\\\\widehat{Value}$, but for our analysis to be useful, we need some way to relate them. Often, the way that we do this is having a couple of functions that translate between the two domains. A common way to notate these is $\\\\alpha$ (stands for \\"abstract\\") and $\\\\gamma$ (stands for concretize). In our case, we could have $\\\\alpha : Value \\\\rightarrow \\\\widehat{Value}$ and $\\\\gamma : \\\\widehat{Value} \\\\rightarrow \\\\mathcal{P}(Value)$. Note that $\\\\gamma$\'s return type is a powerset, since there are any number of values in a given type.\\n\\nOnce we have one or both of these functions, we can write theorems that relate elements of the two domains. After running the abstract analysis and get a result in the abstract domain, then we can say something about the concrete execution of the program. If we can prove the analysis is sound, then the result of the abstract interpreter tells us something trustworthy about what the concrete analysis will look like.\\n\\nTake the program\\n```\\n((lambda x. if0 x then 1 else 2 end) 0)\\n```\\n\\nWe can just interpret this program and know that the result will be `1`, which is in $Value$. But in general, because we have `letrec` in our language, just interpretting and finding the result might not terminate, and in any case might take a very long time. A type check would be much faster. If you\'ve taken CS 330 at BYU, you probably noticed that the `typecheck` function looks very similar to the `interp` function. One difference is the types they return (`Value` vs `Type`), but the more important one is that we know `typecheck` will always terminate, and is much faster.\\n\\nRunning a type check on this program produces `nat`, which is in $\\\\widehat{Value}$. While this is not nearly as specific as `1`, it does tell us something! We can use $(\\\\gamma \\\\, nat)$ to return a set of all of the possibilities our program *could* return, based on our analysis. In this case, we know that it could be any natural number, and that it definitely isn\'t a closure.\\n\\n## Generalizing\\n\\nIn our type check example, our abstract domain was related to the values returned by our program, but in general, there are all kinds of domains we could be concerned with. For instance, if our language has a store, we could have an abstract store that has only finitely many addresses. The key is that whatever abstraction you use, you can prove that that abstraction makes your interpretation decidable."},{"id":"galois-transformers","metadata":{"permalink":"/blog/galois-transformers","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-galois-transformers.md","source":"@site/blog/2024-11-galois-transformers.md","title":"Paper - Galois Transformers and Modular Abstract Interpreters","description":"Galois Transformers and Modular Abstract Interpreters [@daraisgalois2015] provides a way to do abstract interpretation in general, without having to specify a particular language or analysis. This paper is fairly mathematically dense, so I\'ll do my best to break down what I understood from it. This paper is also a good starting point if you want to understand Dr. Germane\'s paper on Full Control-Flow Sensitivity.","date":"2024-11-07T12:00:00.000Z","tags":[{"inline":true,"label":"paper","permalink":"/blog/tags/paper"},{"inline":true,"label":"galois connections","permalink":"/blog/tags/galois-connections"},{"inline":true,"label":"monad transformers","permalink":"/blog/tags/monad-transformers"},{"inline":true,"label":"abstract interpreters","permalink":"/blog/tags/abstract-interpreters"}],"readingTime":7.105,"hasTruncateMarker":true,"authors":[{"name":"Koby Lewis","title":"Author","url":"https://github.com/plyb","imageURL":"https://github.com/plyb.png","key":"koby","page":null},{"name":"Tim Whiting","title":"Author","url":"https://github.com/TimWhiting","imageURL":"https://github.com/TimWhiting.png","key":"tim","page":null}],"frontMatter":{"slug":"galois-transformers","title":"Paper - Galois Transformers and Modular Abstract Interpreters","date":"2024-11-07T12:00","authors":["koby","tim"],"tags":["paper","galois connections","monad transformers","abstract interpreters"]},"unlisted":false,"prevItem":{"title":"Topic - Abstract Interpretation","permalink":"/blog/abstract-interpretation"},"nextItem":{"title":"Paper - Abstracting Abstract Machines","permalink":"/blog/abstracting-abstract-machines"}},"content":"Galois Transformers and Modular Abstract Interpreters [@darais_galois_2015] provides a way to do abstract interpretation *in general*, without having to specify a particular language or analysis. This paper is fairly mathematically dense, so I\'ll do my best to break down what I understood from it. This paper is also a good starting point if you want to understand Dr. Germane\'s paper on Full Control-Flow Sensitivity.\\n\\n\x3c!--truncate--\x3e\\n\\nUltimately, the goal of the paper is basically to have some function (let\'s call it `run_analysis`), that can be passed a collection of parameters that specify the language on which to run an analysis, as well as what analysis should be run. The coolest thing in my mind is that they were able to do this in such a way that as long as you can prove a couple of properties about the arguments you pass to `run_analysis`, the soundness of the analysis is given to you for free.\\n\\n## Breaking Down the Abstract\\n\\nI think a good way to understand what is going on in this paper will be to break down the abstract and understand each of its claims.\\n\\n### The Problem Statement\\n\\n> The design and implementation of static analyzers has become increasingly systematic. Yet for a given language or analysis feature, it often requires tedious and error prone work to implement an analyzer and prove it sound. In short, static analysis features and their proofs of soundness do not compose well, causing a dearth of reuse in both implementation and metatheory.\\n\\nThis part is fairly self explanatory. The authors identified a classic software engineering problem: we\'re repeating ourselves a lot when we write up analyzers and their proofs. Is there some way we can take the parts that get repeated and abstract them out, parameterized on the portions that actually are different between different analyses?\\n\\nNow would probably be a good time to note that the paper uses the word \\"abstract\\" in a couple of different ways. One of them is the traditional programming sense, that means something like \\"generalize\\". If we\'re repeating ourselves, let\'s separate out the portions that are repeated from the portions that are different. The other way it uses abstract is in the term \\"abstract interpreter\\". See [abstract interpretation](./2024-11-abstract-interpretation.md) for more information on what that means, if you\'re unfamiliar with the concept.\\n\\n### Solution\\n\\n> We solve the problem of systematically constructing static analyzers by introducing Galois transformers: monad transformers that transport Galois connection properties.\\n\\n\\"Systematically constructing\\" here just means that they have an algorithm for combining the parameters (the language and analysis properties) to produce an analysis that is always sound, i.e. the `run_analysis` function is defined in the paper.\\n\\n\\"Galois transformers\\" are (as the title suggests) the biggest new concept of the paper. They are defined as \\"monad transformers that transport Galois connection properties\\". Monad transformers are a special kind of function that take in a monad and produce a different monad, preserving the monad laws. The reason that we care about them is they allow us to essentially \\"stack\\" monads. For instance, there is a State Monad Transformer called $ S^t $ (monad transformers are usually written with some symbol followed by a superscript `t`). When $ S^t $ is applied to a different monad (say, the `Option` monad), it \\"adds\\" the state effect to the monad. You can then chain these to combine effects in a highly composable way.\\n\\n\\"Galois connection properties\\" is a whole topic in and of itself that I may write an article on at some point. The TL;DR here though (as far as I understand it) is that if you have two sets, $A$ and $\\\\widehat{A}$, and a \\"Galois connection\\" between them, there is a sort of mapping between them. This mapping is used in several different ways in the paper, which I\'ll get into a little more later on. For now, here is a more formal definition of Galois connections. Given two partial orders, $(A, \\\\leq)$ and $(\\\\widehat{A}, \\\\sqsubseteq)$, and functions $\\\\alpha : A \\\\rightarrow \\\\widehat{A}$ and $\\\\gamma : \\\\widehat{A} \\\\rightarrow A$ for converting between them, then there is a Galois connection between them if for any $a \\\\in A$ and $\\\\widehat{a} \\\\in \\\\widehat{A}$, $a \\\\leq (\\\\gamma \\\\, \\\\widehat{a}) \\\\Leftrightarrow (\\\\alpha \\\\, a) \\\\sqsubseteq \\\\widehat{a}$. Note that the names here are suggestive of concrete and abstract domains, but Galois connections aren\'t specific to abstract interpretation. Intuitively, this definition means that there is a way to convert between the two sets that preserves the partial order.\\n\\nThere are two ways the paper uses Galois connections. One of them is to formalize the correspondence between concrete and abstract domains. See [abstract interpretation](./2024-11-abstract-interpretation.md) for more details of what that means. The other has to do with how they define \\"running\\" the analysis. The paper defines a \\"monadic interpreter\\", which takes an expression of type $Exp$ and returns an expression wrapped in a monad $m(Exp)$. The monad in this case is specific to the particular language and analysis. For reasons that I don\'t fully understand yet, they state that this monadic function \\"cannot be iterated to least-fixed-point to execute the analysis.\\" Instead, they define a transition system that defines a step relation between configurations of type $\\\\Sigma$. They then prove a Galois connection between this relation, $\\\\Sigma \\\\rightarrow \\\\Sigma$ and the monadic function $Exp \\\\rightarrow m(Exp)$. The transition system *can* be used in a least-fixed-point computation. The takeaway seems to be that this allows them to define a concept of execution, even though at first that seems tricky.\\n\\n:::info[Tim]\\nI believe the answer to your question is as follows:\\n\\nThe monadic function \\"cannot be iterated to least-fixed-point to execute the analysis\\" because the monad itself only maps expressions to monadic values.\\nWe need the monad to correspond to a transition system where we can reify the configuration $\\\\Sigma$ (whose components are implicit and modularized in the monad) in such a way that we can see if a particular $\\\\Sigma$ has already been visited. That way we can iterate to a fixed-point by collecting all configurations that have already been seen, and running unseen configurations through the monad to get another set of states.\\n\\nWithout this connection, we wouldn\'t know which configurations still need to be run, and which configurations have been produced by the monad.\\n\\nAbstracting definitional interpreters [@darais_abstracting_2017] shows how you can reify the configurations within the monad and iterate to a fixed-point using another nondeterminism monad and cache.\\nThat paper also does the analysis in big-step style.\\nThis paper however, limits the nondeterminism monad to the inherent nondeterminism due to the abstract domains. \\n:::\\n\\nNow for the key part: they show that there are certain monad transformers (which they call \\"Galois transformers\\") that, when applied, preserve the Galois connections of the monads they are applied to. This is a critical step in what they are trying to accomplish, because it now means that if you apply a series of Galois transformers, as long as your original monad was sound and each Galois transformer was sound, they result is also sound by construction. The proofs compose!\\n\\n### Accomplishments\\n\\n> In concert with a monadic interpreter, we define a library of monad transformers that implement building blocks for classic analysis parameters like context, path, and heap (in)sensitivity. \\n\\nThey also define a few arguments that can be passed to our hypothetical `run_analysis` function. These define things like state and nondeterminism effects for a language, as well as parameterizing the analysis based on [sensitivity](./2024-11-flow-sensitivity.md).\\n\\n> Moreover, these can be composed together independent of the language being analyzed.\\n> Significantly, a Galois transformer can be proved sound once and for all, making it a reusable analysis component. As new analysis features and abstractions are developed and mixed in, soundness proofs need not be reconstructed, as the composition of a monad transformer stack is sound by virtue of its constituents. Galois transformers provide a viable foundation for reusable and composable metatheory for program analysis.\\n\\nThese statements summarize what we\'ve been talking about with the fact that the proofs compose nicely. \\n\\n> Finally, these Galois transformers shift the level of abstraction in analysis design and implementation to a level where non-specialists have the ability to synthesize sound analyzers over a number of parameters.\\n\\nFinally, they make the assertion that, since the individual pieces are so easily composable, someone who is not familiar with the mathematical foundations of program analysis could still create an analyzer, as long as they were given the building blocks. This is a pretty cool assertion, although I\'m not sure they really justify it much."},{"id":"abstracting-abstract-machines","metadata":{"permalink":"/blog/abstracting-abstract-machines","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-aam.md","source":"@site/blog/2024-11-aam.md","title":"Paper - Abstracting Abstract Machines","description":"The Abstracting Abstract Machines [@vanhornabstracting_2010] paper","date":"2024-11-06T12:00:00.000Z","tags":[{"inline":true,"label":"paper","permalink":"/blog/tags/paper"},{"inline":true,"label":"control flow analysis","permalink":"/blog/tags/control-flow-analysis"}],"readingTime":0.165,"hasTruncateMarker":true,"authors":[{"name":"Tim Whiting","title":"Author","url":"https://github.com/TimWhiting","imageURL":"https://github.com/TimWhiting.png","key":"tim","page":null}],"frontMatter":{"slug":"abstracting-abstract-machines","title":"Paper - Abstracting Abstract Machines","date":"2024-11-06T12:00","authors":["tim"],"tags":["paper","control flow analysis"]},"unlisted":false,"prevItem":{"title":"Paper - Galois Transformers and Modular Abstract Interpreters","permalink":"/blog/galois-transformers"},"nextItem":{"title":"Topic - Control Flow Analysis","permalink":"/blog/control-flow-analysis"}},"content":"The Abstracting Abstract Machines [@van_horn_abstracting_2010] paper\\nis frequently cited due to its presentation of a methodological process of\\nabstracting a small-step abstract machine for analysis.\\n\\n\x3c!--truncate--\x3e\\n\\n## Tim - Learnings / Thoughts\\n\\nTODO"},{"id":"control-flow-analysis","metadata":{"permalink":"/blog/control-flow-analysis","editUrl":"https://github.com/byu-static-analysis-lab/blog/tree/main/blog/2024-11-control-flow-analysis.md","source":"@site/blog/2024-11-control-flow-analysis.md","title":"Topic - Control Flow Analysis","description":"This is a Blog for the BYU Static Analysis Lab","date":"2024-11-06T12:00:00.000Z","tags":[{"inline":true,"label":"static analysis","permalink":"/blog/tags/static-analysis"},{"inline":true,"label":"flow analysis","permalink":"/blog/tags/flow-analysis"},{"inline":true,"label":"soundness","permalink":"/blog/tags/soundness"},{"inline":true,"label":"completeness","permalink":"/blog/tags/completeness"},{"inline":true,"label":"Demand CFA","permalink":"/blog/tags/demand-cfa"}],"readingTime":4.47,"hasTruncateMarker":true,"authors":[{"name":"Tim Whiting","title":"Author","url":"https://github.com/TimWhiting","imageURL":"https://github.com/TimWhiting.png","key":"tim","page":null}],"frontMatter":{"slug":"control-flow-analysis","title":"Topic - Control Flow Analysis","date":"2024-11-06T12:00","authors":["tim"],"tags":["static analysis","flow analysis","soundness","completeness","Demand CFA"]},"unlisted":false,"prevItem":{"title":"Paper - Abstracting Abstract Machines","permalink":"/blog/abstracting-abstract-machines"}},"content":"This is a Blog for the BYU Static Analysis Lab\\n\\nDr. Kimball Germane is our advisor at BYU. https://kimball.germane.net/\\n\\nHe has a lot of experience with static analysis, and especially with making control flow analysis practical for real world use.\\n\\nIn this blog post we will introduce static analysis from a few different perspectives.\\n\\nClick (Read More) below to read the rest of the post.\\n\\n\x3c!--truncate--\x3e\\n\\n## Tim\'s Thoughts on Static Control Flow Analysis \\n**Static analysis** is a way to analyze properties about a program without running it. Some properties of a program are restricted by the type system, but\\noften you want to know a bit more about the program than a type system can tell you. For example, you might want to know if an integer is always positive, or if a function never throws an exception.\\n\\nWhile some of these properties can be determined by a more restrictive type system, there is always a tradeoff when you make a type system more restrictive. The more restrictive a type system is, the more difficult it is to write programs that may be valid, but don\'t fit the type system. On the other hand, some type systems are super expressive allowing the programmer to express rich types where types can depend on values (see [Idris](https://www.idris-lang.org/) for an example). Both by making a type system more restrictive and by making it more expressive, you completely change the developer experience which can make it much easier or much harder to write programs.\\n\\nEven with an expressive type system, there are still properties that are interesting to a compiler that the type system is unlikely to help you with. A common example is inlining. Inlining functions can be very beneficial to runtime performance \u2013 it can save you the overhead of a function call (though the overhead can be minimal depending on the language you use). Additionally, in the case of higher order functions such as closures, it can save you de/allocation of the closure. Another example of a property that is useful, but not typically expressed in type systems are security properties such as ensuring that secrets within a program are not leaked, and that the program doesn\'t accept any input from the user that could compromise the program.\\n\\n**Flow analysis** is one way of solving many static analysis questions. Essentially the common questions that we ask is about what values can flow to which points in the program. Due to infinite loops, and arbitrary run-time input, we cannot just interpret the program while logging all values of each variable. Instead we really want to know all possible ways the program can execute given *any* input or path through the program. This is a very difficult problem, and in general is undecidable. However, there are many cases where we can get a good approximation of the answer, and that is what we are interested in. There are many ways of approximating the solution to this problem, but what we are interested in is a *sound* solution. For the solution to be *sound* it means that we need to consider all possible executions of the program. Because it is an approximation we can either underestimate all possible executions (leaving out some possibilities) or overestimate all possible executions (including some possibilities that are in reality not possible). We are interested in the latter case, because we want to be sure that we are not missing any possible execution paths. If we don\'t consider all possible executions that could mean that our analysis is *unsound*, for example it might say that a value is always positive, when in reality it could be negative. You could imagine that it might be useful to know when something has to happen, in which case you would want a *complete* analysis. A complete analysis guarantees that if it says something happens then it really does. Soundness and completeness are not opposites, but they are both useful properties to think about when discussing formal reasoning systems.\\nFlow analysis starts with a sound analysis and much of the research focuses on how to make the analyses more *precise* (complete).\\n\\nHowever just following where simple values flow \u2013 such as into and out of functions or into branches of a switch or if statement \u2013 is not typically enough. Many languages have first class functions. With first class functions, functions are values that themselves can be passed around the program. This means that the flow of a value can depend on the flow of a function and vice versa. This is what we call **Control Flow Analysis**.\\n\\nControl flow analyses are really expensive, so in practice, they are not often used in compilers. Our aim is to change that. There are theoretic barriers indicating that a full-program flow analysis requires at least $O(n^3)$ time / space. However we believe that there is a lot of room for improvement in the practicality of control flow analyses, by breaking up the full program into parts of interest and doing a local control flow analysis on demand from there. We believe that this approach can solve many of the flow problems that compilers and users would like answered.\\n\\nA great paper by Kimball on the topic is [Demand Control-Flow Analysis](http://kimball.germane.net/germane-2019-dcfa.pdf). It is a great read, especially for understanding the approach we are using for part ofmy PhD research."}]}}')}}]);